{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnsgNW8nD+vNEpOhEVig+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp0088/colab-wip/blob/main/comp88_lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP0088 Lab Assignment 6\n",
        "\n"
      ],
      "metadata": {
        "id": "iZgl8O7b-fi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        " In general it is rather unlikely that you will want to implement neural network models from scratch, as you did in week 5. Instead you will typically use components provided by a **deep learning framework**. In this week's exercises you will do just that, building several different neural network models using [PyTorch](https://pytorch.org/). These models will also be tested on somewhat more complex data than in previous weeks, doing **image classification** on several standard datasets.\n",
        "\n",
        " Depending on the configuration options (which you will probably want to play around with), you are likely to want to use a VM with a GPU for this notebook, rather than the default CPU VM. You can control this using the \"Change runtime type\" option in the **Connect** popup menu in the upper right of the UI.\n",
        "\n"
      ],
      "metadata": {
        "id": "N12ZHil1_ZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "hxzyJ3xeT4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This week's imports are a bit more extensive than usual — in addition to the typical NumPy & Matplotlib libraries we will also need deep learning functionality from PyTorch."
      ],
      "metadata": {
        "id": "4vHvSz5pReci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL8UJ7lgLznk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "from time import perf_counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, fetch the shared COMP0088 lab code from the module GitHub:"
      ],
      "metadata": {
        "id": "K1RLN5QATflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load lab code and resources\n",
        "!git clone https://github.com/comp0088/shared.git comp0088\n",
        "\n",
        "# at the moment this is all we care about\n",
        "import comp0088.utils as utils"
      ],
      "metadata": {
        "id": "v3X7LDC5KAob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also as usual, set up some basic items for later use."
      ],
      "metadata": {
        "id": "t1rjWDV14cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up some items for use in later code\n",
        "shared_rng = numpy.random.default_rng()\n",
        "\n",
        "# in practice we will extract this from the data, but\n",
        "# we'll set up some values here to use as defaults\n",
        "INPUT_SHAPE = [1, 28, 28]\n",
        "INPUT_SIZE = np.prod(INPUT_SHAPE)\n",
        "\n",
        "# default layer configurations for the different model types\n",
        "DEFAULT_MLP = [32, 32]\n",
        "DEFAULT_CNN = [16, 16]\n",
        "DEFAULT_RNN = [64, 1]\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "DEFAULT_DATA = 'USPS'\n",
        "\n",
        "# use CUDA GPU acceleration if it's available\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "PfZQlfuELVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, we're going to set up a couple more bits of infrastructure for the rest of the assignment."
      ],
      "metadata": {
        "id": "FV8caPCebbfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model superclass\n",
        "\n",
        "First of all, we provide a very simple PyTorch model to act as the base class for the models you'll define in later tasks. This is not just an empty placeholder, it works as a real model, albeit a very simple one. Your own classes will look similar, just a little more complex."
      ],
      "metadata": {
        "id": "-cMX-BLMY2oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Trivial PyTorch linear model to act as a placeholder\n",
        "    and parent for the different task classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input=INPUT_SIZE, output=NUM_CLASSES, create_layers=True):\n",
        "        \"\"\"\n",
        "        Initialise the model.\n",
        "\n",
        "        The data passed to all the models this week is *image* data.\n",
        "        Here we flatten it into a vector and apply a single linear\n",
        "        layer with no activation.\n",
        "\n",
        "        # Arguments\n",
        "            input: the input size for the linear layer\n",
        "            output: the output size for the linear layer\n",
        "            create_layers: whether to actually create the layers\n",
        "                -- subclasses will do their own thing instead\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential( nn.Flatten(), nn.Linear(input, output) ) if create_layers else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Execute a forward pass of the model.\n",
        "\n",
        "        # Arguments\n",
        "            x: the input data to the first layer\n",
        "\n",
        "        # Returns\n",
        "            output tensor from the last layer\n",
        "        \"\"\"\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "i6FVnCe_Y3H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading\n",
        "\n",
        "Training and evaluating image classifiers requires image data. The PyTorch package `torchvision.datasets` provides access to a number of standard image datasets. We define a simple wrapper for that which loads one of the following five datasets:\n",
        "\n",
        "* [MNIST](http://yann.lecun.com/exdb/mnist/): Classic dataset of 28×28 handwritten digits. Extremely widely used, but also widely considered to be flawed.\n",
        "* [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist): Dataset of garment images, designed as a drop-in replacement for MNIST that avoids some of its drawbacks.\n",
        "* [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist): Dataset of 10 handwritten Japanese characters, again intended as a drop-in replacement for MNIST.\n",
        "* [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html): dataset of small (32×32) colour photographic images of 10 classes (6 types of animal and 4 of vehicle)\n",
        "* [USPS](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps): Handwritten digit dataset using much smaller (8×8) greyscale images. (We default to this because it is less resource hungry and hence quicker to work with.)\n",
        "\n",
        "Our wrapper also allows using only a subset of the image classes from the set — so for example you could choose to use only the first 4 digits from MNIST or USPS by specifying `num_classes=4`."
      ],
      "metadata": {
        "id": "wcoo2WiWcBkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset( dataset=DEFAULT_DATA, num_classes=NUM_CLASSES ):\n",
        "  \"\"\"\n",
        "  Load both the training and validation portions of a standard\n",
        "  image dataset, optionally limiting to only a subset of classes.\n",
        "\n",
        "  Known datasets are: 'MNIST', 'FashionMNIST', 'Kuzushiji-MNIST',\n",
        "  'CIFAR10' and 'USPS'. If an unknown dataset is specified we\n",
        "  default to 'USPS'.\n",
        "\n",
        "  # Arguments:\n",
        "    dataset: the name of the dataset to load.\n",
        "             NB: only the first letter is actually used, case insensitive.\n",
        "    num_classes: how many of the image classes in the dataset to\n",
        "             include.\n",
        "\n",
        "  # Returns:\n",
        "    train_data: a PyTorch Tensor containing the training data\n",
        "    val_data: a PyTorch Tensor containing the validation data\n",
        "    name: the display name of the dataset\n",
        "    num_classes: the number of classes loaded (this will default\n",
        "          to 10 if the specified value is out of bounds)\n",
        "\n",
        "  \"\"\"\n",
        "  data, name = {\n",
        "    'm' : (datasets.MNIST, 'MNIST'),\n",
        "    'f' : (datasets.FashionMNIST, 'FashionMNIST'),\n",
        "    'k' : (datasets.KMNIST, 'Kuzushiji-MNIST'),\n",
        "    'c' : (datasets.CIFAR10, 'CIFAR10'),\n",
        "  }.get(dataset.lower()[:1], (datasets.USPS, 'USPS'))\n",
        "\n",
        "  train_data = data('data', train=True, download=True, transform=ToTensor())\n",
        "  val_data = data('data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "  if (num_classes > 1) and (num_classes < NUM_CLASSES):\n",
        "    train_data = Subset(train_data, np.flatnonzero(np.array(train_data.targets) < num_classes))\n",
        "    val_data = Subset(val_data, np.flatnonzero(np.array(val_data.targets) < num_classes))\n",
        "  else:\n",
        "    num_classes = NUM_CLASSES\n",
        "\n",
        "  return train_data, val_data, name, num_classes"
      ],
      "metadata": {
        "id": "S36E_QmYgr_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's actually load a dataset and display some random example images from it.\n",
        "\n",
        "Note that the data loaded here will be used throughout the rest of the notebook unless you explicitly load something different somewhere else. To try out a different dataset just change the name in the `load_dataset` call below and rerun this cell."
      ],
      "metadata": {
        "id": "k0zTdJ49mkuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load image data\n",
        "train_data, val_data, name, num_classes = load_dataset('USPS')\n",
        "\n",
        "# plot some samples from the training set\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "utils.plot_image(ax, utils.torch_data_to_image_grid(train_data, 10, 10, shuffle=True, rng=shared_rng),\n",
        "                     title=f'{name} Data Examples')"
      ],
      "metadata": {
        "id": "SgpDJ7HvnTh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training curves\n",
        "\n",
        "Finally, we'll provide a simple plotting function to plot the improvement (hopefully) of loss and accuracy over the course of iterative training. There's nothing here you haven't seen before, but we'll be doing the same thing many times so it makes sense to stick it in a function."
      ],
      "metadata": {
        "id": "bUYE4Nvl6Dwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training (train_loss, train_acc, val_loss, val_acc, model_name):\n",
        "  \"\"\"\n",
        "  Plot how loss and accuracy change over training.\n",
        "  \"\"\"\n",
        "\n",
        "  # create a figure & axes to plot into\n",
        "  fig = plt.figure(figsize=(9, 4.5))\n",
        "  axs = fig.subplots(nrows=1, ncols=2)\n",
        "\n",
        "  # choose colours\n",
        "  blue, orange = plt.cm.tab10.colors[:2]\n",
        "\n",
        "  # we need to actually have something to plot\n",
        "  if train_loss[0] is None:\n",
        "      utils.plot_unimplemented(axs[0], title=f'Loss - {model_name}')\n",
        "      utils.plot_unimplemented(axs[1], title=f'Accuracy - {model_name}')\n",
        "  else:\n",
        "      # x axis for both plots will be the epochs\n",
        "      epochs = np.arange(1, len(train_loss)+1)\n",
        "\n",
        "      # plot the losses\n",
        "      axs[0].plot(epochs, train_loss, color=blue, label='Training')\n",
        "      axs[0].plot(epochs, val_loss, color=orange, label='Validation')\n",
        "      axs[0].set_xlabel('Epoch')\n",
        "      axs[0].set_ylabel('Loss')\n",
        "      axs[0].set_title(f'Loss - {model_name}')\n",
        "      axs[0].legend()\n",
        "\n",
        "      # plot the accuracies\n",
        "      axs[1].plot(epochs, train_acc, color=blue, label='Training')\n",
        "      axs[1].plot(epochs, val_acc, color=orange, label='Validation')\n",
        "      axs[1].set_xlabel('Epoch')\n",
        "      axs[1].set_ylabel('Accuracy')\n",
        "      axs[1].set_title(f'Accuracy - {model_name}')\n",
        "      axs[1].legend()\n",
        "\n",
        "  fig.tight_layout(pad=1)"
      ],
      "metadata": {
        "id": "6N0BKCnt6Env"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Training and Testing\n",
        "\n",
        "You're going to put together a number of different PyTorch models in this assignment, but the processes of training and testing are essentially the same for all of them, so you'll start by implementing those.\n",
        "\n",
        "The basic steps for training and testing are the same as in your hand-rolled MLP, and PyTorch will take care of a lot of the underlying fiddliness for you. But as with any framework, you need to get to grips with the classes and functions provided, and the idioms the framework expects you to use.\n",
        "\n",
        "Both for training a PyTorch model and for testing it, you need to run through the data in mini-batches, processing each in turn. The data will be provided already wrapped in a PyTorch DataLoader object , which you can iterate over directly:\n",
        "\n",
        "```python\n",
        "for X, y in dataloader:\n",
        "\t# ... do stuff\n",
        "```\n",
        "\n",
        "If you are using an external processor like a GPU, you'll want to make sure that the data batches are copied to it. You should do this by calling `.to(DEVICE)` on the Tensor objects:\n",
        "\n",
        "```python\n",
        "\tX = X.to(DEVICE)\n",
        "\ty = y.to(DEVICE)\n",
        "```\n",
        "\n",
        "(Note: the global variable `DEVICE` here was initialised back in the **Setting Up** section. It's just a string, whose value is either `cuda` or `cpu`. Even if you aren't using a GPU it is sensible to make this call, in case you wind up using one later.)\n",
        "\n",
        "Many model layers behave differently during training and at test time—for example **dropout** layers only switch neurons off during training. So before invoking a model you should tell it which mode it is in. To set it to training mode call `model.train()`, to set it to evaluation mode call `model.eval()`.\n",
        "\n",
        "To do a **forward pass** through the model and make predictions, you just invoke it on the data, as if it was a simple function:\n",
        "\n",
        "```python\n",
        "preds = model(X)\n",
        "```\n",
        "\n",
        "You can then invoke the **loss function** on the predictions:\n",
        "\n",
        "```python\n",
        "loss = loss_function(preds, y)\n",
        "```\n",
        "\n",
        "As mentioned in lecture 6.4, although PyTorch objects look like functions or data, they are actually proxy objects that participate in operation graphs behind the scenes, potentially on some external compute resource such as a GPU. To actually get at the data you need to call an accessor function, such as `.item()`:\n",
        "\n",
        "```python\n",
        "total_loss += loss.item()\n",
        "```\n",
        "\n",
        "(You can also convert Tensors to NumPy arrays with `.numpy()`.)\n",
        "\n",
        "When you call the loss function on the predictions, the loss you get back is a Tensor connected to the end of the operation graph for your model. To perform the **backward pass**, you just call `backward()` on this loss:\n",
        "\n",
        "```python\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "This will propagate all the way back through the model, calculating the gradients throughout. Before doing this, you should call `zero_grad()` on the optimiser to start from a clean slate. After the backward pass you tell the optimiser to **update the parameters** by calling `optimiser.step()`.\n",
        "\n",
        "When evaluating, you don't need the model to calculate gradients because you won't be updating the parameters. You can tell it not to by wrapping it in a `no_grad` context manager:\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "\t# ... do stuff\n",
        "```\n",
        "\n",
        "Training even a single model epoch can take time, so you may want to print progress messages along the way just to reassure yourself that it is doing something."
      ],
      "metadata": {
        "id": "FXpzvXtJAr4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Train a PyTorch model\n",
        "\n",
        "Implement the `train_epoch` function in the cell below.\n",
        "\n",
        "Note that the return values here should be ordinary python numbers, not PyTorch tensors."
      ],
      "metadata": {
        "id": "RGJxos1yA34M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, loss_function, optimiser):\n",
        "    \"\"\"\n",
        "    Train a model on a single epoch of data.\n",
        "\n",
        "    # Arguments\n",
        "        model: a pytorch model (an nn.Module subclass, eg our base Model)\n",
        "        dataloader: a pytorch dataloader that will iterate over the dataset\n",
        "            in batches\n",
        "        loss_function: a pytorch loss function tensor\n",
        "        optimiser: optimiser to use for training\n",
        "\n",
        "    # Returns\n",
        "        loss: mean loss over the whole epoch\n",
        "        accuracy: mean prediction accuracy over the epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "tbnDPv-S6pHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Evaluate a PyTorch model\n",
        "\n",
        "Implement the `test_epoch` function in the cell below.\n",
        "\n",
        "Again, the return values should be numbers, not tensors."
      ],
      "metadata": {
        "id": "diDAHi3gF1uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_epoch(model, dataloader, loss_function):\n",
        "    \"\"\"\n",
        "    Evaluate a model on a dataset.\n",
        "\n",
        "    # Arguments\n",
        "        model: a pytorch model (an nn.Module subclass, eg our base Model)\n",
        "        dataloader: a pytorch dataloader that will iterate over the dataset\n",
        "            in batches\n",
        "        loss_function: a pytorch loss function tensor\n",
        "\n",
        "    # Returns\n",
        "        loss: mean loss over the whole epoch\n",
        "        accuracy: mean prediction accuracy over the epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "ZeOh5-PzF3RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3: Perform multi-epoch training\n",
        "\n",
        "To actually train a model, we'll want to loop over training for many epochs. But we also want to track our progress, so after each epoch of training we'll evaluate the model on our validation set and see how the loss and accuracy change on both sets as we progress. This may allow us to spot overfitting, for example, if the training and validation losses diverge.\n",
        "\n",
        "Implement the `train_loop` model in the cell below to perform this alternate training and testing over multiple epochs. (You will probably find it helpful to print some outputs as you go so you know your function is actually doing something.)"
      ],
      "metadata": {
        "id": "tyPrXmrP0FhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop ( model, train_loader, val_loader, loss_function, optimiser, epochs=10 ):\n",
        "  \"\"\"\n",
        "  Train a model for multiple epochs, evaluating on the validation data every step.\n",
        "\n",
        "  # Arguments\n",
        "      model: a pytorch model (an nn.Module subclass, eg our base Model)\n",
        "      train_loader: a pytorch dataloader that will iterate over the training data\n",
        "      train_loader: a pytorch dataloader that will iterate over the validation data\n",
        "      loss_function: a pytorch loss function tensor\n",
        "      optimiser: optimiser to use for training\n",
        "      epochs: how many epochs to train for\n",
        "\n",
        "  # Returns\n",
        "      train_loss: a list of loss values on the training set for each epoch\n",
        "      train_acc: a list of the accuracies on the training set for each epoch\n",
        "      val_loss: a list of the loss values on the validation set for each epoch\n",
        "      val_acc: a list of the accuracies on the validation set for each epoch\n",
        "  \"\"\"\n",
        "\n",
        "  # TODO: implement this\n",
        "  return [], [], [], []"
      ],
      "metadata": {
        "id": "ikmYPp2X0Sny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 1\n",
        "\n",
        "Execute the code cell below to use the functions you wrote above to train and evaluate a simple linear model.\n",
        "\n",
        "As mentioned earlier, we're going to be using the same training process repeatedly in this lab, so it's wrapped in a function here. Note that a number of fitting hyperparameters are specified as function arguments. Feel free to play around with these (here and in later tasks) to get a feel for how they affect performance."
      ],
      "metadata": {
        "id": "NPwNjtpYh_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_training ( model, model_name,\n",
        "\n",
        "                       # data\n",
        "                       train=train_data, val=val_data,\n",
        "\n",
        "                       # training configuration\n",
        "                       batch=25, epochs=10,\n",
        "\n",
        "                       # optimiser configuration\n",
        "                       lr=1e-2, decay=0, momentum=0.9 ):\n",
        "  \"\"\"\n",
        "  Train and evaluate a PyTorch model using the provided data, plotting\n",
        "  results at the end. Uses a standard SGD optimiser and cross entropy loss.\n",
        "\n",
        "  # Arguments\n",
        "      model: a PyTorch model (an nn.Module subclass, eg our base Model)\n",
        "      model_name: display name for the module (used on the final plot)\n",
        "      train: a PyTorch Tensor of training data, as loaded by `load_dataset`\n",
        "      val: a PyTorch Tensor of validation data, as loaded by `load_dataset`\n",
        "      batch: batch size for the mini-batch training\n",
        "      epochs: number of epochs to train for\n",
        "      lr: learning rate for the optimiser\n",
        "      decay: weight decay (L2 regularisation) for the optimiser\n",
        "      momentum: momentum for the optimiser\n",
        "\n",
        "  \"\"\"\n",
        "  # create data loaders\n",
        "  train_loader = DataLoader(train, batch_size=batch)\n",
        "  val_loader = DataLoader(val, batch_size=batch)\n",
        "\n",
        "  # this is a classification problem, so we'll use a crossentropy loss\n",
        "  # (NB: by default PyTorch applies softmax squashing to the inputs to this,\n",
        "  # so there is no need for a separate explicit softmax activation)\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  # and a basic gradient descent optimiser\n",
        "  optimiser = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "\n",
        "  # train for specified number of epochs\n",
        "  train_loss, train_acc, val_loss, val_acc = train_loop(model, train_loader, val_loader, loss_function, optimiser, epochs=epochs)\n",
        "\n",
        "  # finally, let's plot how it went\n",
        "  plot_training(train_loss, train_acc, val_loss, val_acc, model_name)\n",
        "\n",
        "\n",
        "# in this case we're just creating a default linear model,\n",
        "# since you haven't implemented any others yet\n",
        "input_shape = tuple(train_data[0][0].shape)\n",
        "input_size = np.product(input_shape)\n",
        "model = Model(input=input_size, output=num_classes).to(DEVICE)\n",
        "model_name = f'Linear [{input_size}×{num_classes}]'\n",
        "\n",
        "# train it and plot the results\n",
        "perform_training(model, model_name)\n"
      ],
      "metadata": {
        "id": "m5XyX-uSFWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Build a Multi-Layer Perceptron\n",
        "\n",
        "All of the models in this session are going to have a pretty similar structure: they'll be classes inheriting from the `Model` class defined earlier in **Setting Up** (and in turn from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)).\n",
        "\n",
        "Each will have an `__init__`  method to initialise the layers and a `forward` method to perform the forward pass. (The backward pass will take care of itself through the magic of [automatic differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).) However, in this case the `forward` method can be exactly the same as that for `Model`, so you can just inherit that.\n",
        "\n",
        "The MLP will do essentially the same as what you implemented by hand last week, but built out of PyTorch layers — specifically, [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) and [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html). You can wrap these in an [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model. As noted in the docstring, you will need to use [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) to turn the image inputs into simple vectors. (This same process is used in the `Model` class defined earlier, so you may find it helpful to look at that.)\n",
        "\n",
        "Assign your `Sequential` model to the instance variable `self.layers`, replacing the value set in the superclass, so that `forward` will use it."
      ],
      "metadata": {
        "id": "dQRxCl6s-siV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Model):\n",
        "    \"\"\"\n",
        "    Simple multi-layer perceptron model for image classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input=INPUT_SIZE, spec=DEFAULT_MLP, output=NUM_CLASSES):\n",
        "        \"\"\"\n",
        "        Initialise the multi-layer perceptron with the specified arrangement\n",
        "        of fully-connected layers, using ReLU activation on each layer.\n",
        "\n",
        "        Note that the data passed to this model (as for all models this\n",
        "        week) will be *image* data, so the first layer in the model\n",
        "        should be a Flatten layer, to turn the C * W * H pixel arrays\n",
        "        into one dimensional ones.\n",
        "\n",
        "        # Arguments\n",
        "            input: the input size for the first hidden layer\n",
        "            spec: a list of sizes for the intermediate layers\n",
        "            output: the output size for the final layer\n",
        "        \"\"\"\n",
        "        super().__init__(input, output, create_layers=False)\n",
        "\n",
        "        # TODO: implement this"
      ],
      "metadata": {
        "id": "yq5o_CFJ-3fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 2\n",
        "\n",
        "Execute the cell below to train and evaluate an MLP model. The hidden layer configuration is specified by `MLP_SPEC` and training hyperparameters are passed as arguments to `perform_training` — you may want to play around with these.\n",
        "\n",
        "How does the MLP perform? What happens if you repeat the test multiple times? Can it beat the accuracy of the simple linear model? Does it do so consistently?"
      ],
      "metadata": {
        "id": "FyYQEAdfQJP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an MLP\n",
        "MLP_SPEC = [32]\n",
        "input_shape = tuple(train_data[0][0].shape)\n",
        "input_size = np.product(input_shape)\n",
        "\n",
        "model = MLP(input=input_size, spec=MLP_SPEC, output=num_classes).to(DEVICE)\n",
        "model_name = f'MLP {MLP_SPEC}'\n",
        "\n",
        "perform_training(model, model_name, batch=25, epochs=10, lr=1e-2, momentum=0.9, decay=0)"
      ],
      "metadata": {
        "id": "-23s09alQR1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Build a Convolutional Neural Network\n",
        "\n",
        "Convolutional neural networks apply a number of learned *filters* or *convolution kernels* at equally-spaced locations across the data. They are very well-suited to image processing, providing the property of **translational equivariance**: the same patch of pixels will produce the same response wherever it is in the image.\n",
        "\n",
        "Convolutional layers are defined to match the spatial or temporal dimensionality of the target data: 1D for single stream data like audio, 2D for data with a planar spatial structure like images, 3D for volumetric data, and so on. In this case we will be using image data, so you will need to use 2D layers, which are provided by the PyTorch [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) class.\n",
        "\n",
        "CNNs can have pretty complex layer structures, but here we'll keep things simple. All layers will use the same **kernel size**, **padding** and **stride**, and the same (ReLU) activation function. The supplied `spec` argument is just a list defining how many filter channels there are at each layer.\n",
        "\n",
        "After all the convolutional layers, you'll need to add a final fully-connected (linear) layer to produce the number of class prediction logits specified by `output`. Use the calculation from the lectures to keep track of the output dimensions of the convolutional layers so that you'll know how many outputs there are from the last one. Flatten them and feed the resulting vector into your linear layer. Once again, you should wrap all your layers in a Sequential model.\n",
        "\n",
        "As in the previous task, you can reuse the superclass implementation of `forward`, so you only need to implement `__init__`. Just remember to assign your model to `self.layers`.\n"
      ],
      "metadata": {
        "id": "DWC2YctM_AEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(Model):\n",
        "    \"\"\"\n",
        "    Simple convolutional neural network model for image classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input=INPUT_SHAPE, spec=DEFAULT_CNN, output=NUM_CLASSES,\n",
        "                 kernel=3, stride=2, padding=1):\n",
        "        \"\"\"\n",
        "        Initialise the CNN with the specified arrangement of 2D convolutional\n",
        "        layers and ReLU activations, with a final fully-connected layer to\n",
        "        do the classification.\n",
        "\n",
        "        # Arguments\n",
        "            input: the input shape for the first convolutional layer\n",
        "            spec: a list of numbers of channels for the convolutional layers\n",
        "            output: the output size for the final fully-connected layer\n",
        "            kernel: kernel size for all layers\n",
        "            stride: convolution stride for all layers\n",
        "            padding: amount of padding to add around each layer before convolving\n",
        "        \"\"\"\n",
        "        super().__init__(np.prod(input), output, create_layers=False)\n",
        "        assert(len(spec) > 0)\n",
        "\n",
        "        # TODO: implement this"
      ],
      "metadata": {
        "id": "S0Hzh71w_IH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 3\n",
        "\n",
        "Execute the cell below to train and evaluate an CNN model. The filter configuration is specified by `CNN_SPEC` and training hyperparameters are again passed as arguments to `perform_training` in case want to play around with them.\n",
        "\n",
        "How does the CNN perform? What happens if you repeat the test multiple times? Can it beat the accuracy of the simple linear model and/or the MLP? Does it do so consistently?"
      ],
      "metadata": {
        "id": "IVRK0aN_cM5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_SPEC = [16,16]\n",
        "input_shape = tuple(train_data[0][0].shape)\n",
        "\n",
        "# create the model\n",
        "model = CNN(input=input_shape, spec=CNN_SPEC, output=num_classes, kernel=3, stride=2, padding=1).to(DEVICE)\n",
        "model_name = f'CNN {CNN_SPEC}'\n",
        "\n",
        "# train and evaluate it\n",
        "perform_training(model, model_name, batch=25, epochs=10, lr=1e-2, momentum=0.9, decay=0)"
      ],
      "metadata": {
        "id": "HoFQrEGucaQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Implement Recurrent Neural Networks\n",
        "\n",
        "Recurrent neural networks impose a sequential structure on the data, carrying a memory of earlier processing steps forward through the sequence.\n",
        "\n",
        "PyTorch includes three standard recurrent layers that you can include in your models: a vanilla recurrent unit ([nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)), a **long short-term memory** unit ([nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)) and a **gated recurrent unit** ([nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)). All three have very similar interfaces and can be used interchangeably here. You should choose which kind to create according to the `unit_type` argument.\n",
        "\n",
        "Image data can be considered as a kind of sequence, eg as a sequence of pixels or a sequence of rows. (Other schemes are also possible if you feel like experimenting.) But the original image data is not in that format, so you will need to **reshape** it in order to match the expected structure, converting each 3D image shape (channels $\\times$ height $\\times$ width) into a 2D sequence one (steps $\\times$ features). If you treat single pixels as tokens, then the sequence length will be the number of pixels in the image; if you treat rows as tokens, then the sequence will be the number of rows.\n",
        "\n",
        "You'll need to specify the input size (ie, the size of a single sequence token) when you create the recurrent unit, basing it on the supplied `input` argument and your reshaping strategy. You'll then need to do the actual reshaping in your `forward` method, before sending the data to the RNN. (As a result, in this case you can't just fall back on the superclass implementation.) You may also find it helpful to specify `batch_first=True` when creating the unit.\n",
        "\n",
        "As with the CNN, you'll also need to pass the RNN outputs through a fully connected layer to generate the `output` class logits. Again, there will be some reshaping required here, so in this case it makes sense to have two separate model chunks, rather than trying to pack everything into a Sequential model.\n"
      ],
      "metadata": {
        "id": "_5S2la1L_Rm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(Model):\n",
        "    \"\"\"\n",
        "    Simple recurrent network model for image classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input=INPUT_SHAPE, spec=DEFAULT_RNN, output=NUM_CLASSES,\n",
        "                 unit_type='lstm'):\n",
        "        \"\"\"\n",
        "        Initialise the RNN with the specified stack of recurrent layers,\n",
        "        with a final fully-connected layer to do the classification.\n",
        "\n",
        "        # Arguments\n",
        "            input: the input shape for the image data.\n",
        "                we assume [C, H, W] ordering and will convert into\n",
        "                a sequence ofsequence of H inputs of size C*W\n",
        "            spec: a list specifying the hidden size of each layer (at element 0)\n",
        "                and optionally the number of such layers to stack (at element 1)\n",
        "            output: the output size for the final fully-connected layer\n",
        "            unit_type: what type of recurrent layers to use\n",
        "                'lstm': use nn.LSTM\n",
        "                'gru': use nn.GRU\n",
        "                (anything else): use nn.RNN\n",
        "        \"\"\"\n",
        "        super().__init__(np.prod(input), output, create_layers=False)\n",
        "\n",
        "        # TODO: implement this\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Execute a forward pass of the model.\n",
        "\n",
        "        # Arguments\n",
        "            X: the input data tensor to go into the RNN.\n",
        "               This will be image data with dimension ordering\n",
        "                 N x C x H x W\n",
        "\n",
        "        # Returns\n",
        "            output tensor from the final linear layer\n",
        "        \"\"\"\n",
        "        # TODO: implement this\n",
        "        return None"
      ],
      "metadata": {
        "id": "cIXT5616_YIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 4\n",
        "\n",
        "Execute the cell below to train and evaluate an RNN model. The hidden state size and number of layers are specified by `RNN_SPEC` and training hyperparameters are again passed as arguments to `perform_training` in case want to play around with them.\n",
        "\n",
        "How does the RNN perform? What happens if you repeat the test multiple times? Can it beat the accuracy of the other models? Does it do so consistently?"
      ],
      "metadata": {
        "id": "OW5L0t8Nel6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_SPEC = [64,1]\n",
        "input_shape = tuple(train_data[0][0].shape)\n",
        "\n",
        "model = RNN(input=input_shape, spec=RNN_SPEC, output=num_classes, unit_type='lstm').to(DEVICE)\n",
        "model_name = f'RNN {RNN_SPEC}'\n",
        "\n",
        "perform_training(model, model_name, batch=25, epochs=10, lr=1e-2, momentum=0.9, decay=0)"
      ],
      "metadata": {
        "id": "E_gl0UY2emXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further exploration\n",
        "\n",
        "The main exploration I'd suggest this week is just trying out different model variations and hyperparameters with the different datasets. Do some consistently perform better than others? Are any of these surprising?\n",
        "\n",
        "But, if you've exhausted all the basic variations, consider expanding your models to support additional functionality or incorporate new kinds of layers. For example:\n",
        "\n",
        "* Add **pooling** layers (eg [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)) between convolutions in your CNN to collect features over larger regions.\n",
        "* Allow different filter sizes, strides and padding for each layer in your CNN.\n",
        "* Add **dropout** layers as a form of regularisation. (Use [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) for fully connected layers and [nn.Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html) for convolutional layers. For recurrent layers, the layer [constructor](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) includes an argument `dropout` for adding dropout layers internally.)\n",
        "* The main PyTorch layer types are documented [here](https://pytorch.org/docs/stable/nn.html). Can you think of any other interesting uses for them?\n",
        "\n",
        "If you're feeling super ambitious, you could try building an **autoencoder** (AE) model using the same image datasets used here for classification. We've only mentioned AEs in passing in the lectures so far — they'll be covered more fully in week 9, but there are no lab exercises that week. The key features you'd need to consider in implementing an AE here are:\n",
        "\n",
        "* Instead of using the image label from the dataset as the target variable `y`, you should instead use the image data `X` itself. This is what you want the model to learn to reconstruct.\n",
        "* Your loss function should be something that captures the (dis)similarity of the input and output images. This is more of a regression problem than one of classification, so something like a squared error loss ([nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)) might be appropriate.\n",
        "* You probably want to make sure that the encoder side of your model steps down to a small code layer that the decoder side then expands. Why? What might you expect to happen without such a bottleneck?"
      ],
      "metadata": {
        "id": "AtFoceIOdtmB"
      }
    }
  ]
}